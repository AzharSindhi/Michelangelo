model:
  target: michelangelo.models.asl_diffusion.clip_asl_diffuser_pl_module.ClipASLDiffuser
  params:
    first_stage_config:
      target: michelangelo.models.tsal.asl_pl_module.AlignedShapeAsLatentPLModule
      params:
        ckpt_path: "checkpoints_runs/pointnetvae_dino_contrast_dino_contrastive_20250626-1050_hkze3vhy/last.ckpt"
        shape_module_cfg:
          target: michelangelo.models.pointnet_vae.pointnet2_vae.PointNet2CloudCondition
          params:
            model_name: "pointnet"
            in_fea_dim: 3
            partial_in_fea_dim: 0
            out_dim: 3
            include_t: false
            t_dim: 128
            "model.use_xyz": true
            attach_position_to_input_feature: true
            include_abs_coordinate: true
            include_center_coordinate: true
            record_neighbor_stats: false
            bn_first: false
            bias: true
            res_connect: true
            map_type: "cross_attention"
            condition_loss: true
            image_fusion_strategy: "none"
            include_class_condition: false
            image_backbone: "none"
            use_cross_conditioning: false
            num_class: 256
            class_condition_dim: 128
            gamma: 0.5
            scale_factor: 1
            bn: true
            include_local_feature: true
            include_global_feature: true
            global_feature_remove_last_activation: false
            pnet_global_feature_architecture:
              - [3, 128, 256]
              - [512, 1024]
            attention_setting:
              use_attention_module: true
              attention_bn: true
              transform_grouped_feat_out: true
              last_activation: true
              add_attention_to_FeatureMapper_module: true
            architecture:
              npoint: [2048, 512, 128, 32]
              radius: [0.1, 0.2, 0.4, 0.8]
              neighbor_definition: "radius"
              nsample: [32, 32, 32, 32]
              feature_dim: [128, 128, 256, 256, 512]
              mlp_depth: 3
              decoder_feature_dim: [128, 128, 256, 256, 512]
              include_grouper: false
              decoder_mlp_depth: 2
              use_knn_FP: true
              K: 8
            condition_net_architecture:
              npoint: [2048, 512, 128, 32]
              radius: [0.1, 0.2, 0.4, 0.8]
              neighbor_definition: "radius"
              nsample: [32, 32, 32, 32]
              feature_dim: [128, 128, 256, 256, 512]
              mlp_depth: 3
              decoder_feature_dim: [128, 128, 256, 256, 512]
              include_grouper: false
              decoder_mlp_depth: 2
              use_knn_FP: true
              K: 8
            feature_mapper_architecture:
              neighbor_definition: "radius"
              encoder_feature_map_dim: [32, 32, 64, 64]
              encoder_mlp_depth: 2
              encoder_radius: [0.1, 0.2, 0.4, 0.8]
              encoder_nsample: [32, 32, 32, 32]
              decoder_feature_map_dim: [32, 32, 64, 64, 128]
              decoder_mlp_depth: 2
              decoder_radius: [0.1, 0.2, 0.4, 0.8, 1.6]
              decoder_nsample: [32, 32, 32, 32, 32]
            clip_processor:
              model_name: "clip_processor"
              clip_model_name: "clip_vit_b_32"
              clip_model_path: ""
              clip_dim: 512
              class_names: ["bathtub", "bed", "chair", "desk", "dresser", "monitor", "night_stand", "sofa", "table", "toilet"]

        aligned_module_cfg:
          target: michelangelo.models.tsal.dino_asl_module.DinoAlignedShapeAsLatentModule
          params:
            clip_model_version: "facebook/dinov2-large"
            use_contrastive: true
        
        numpoints: 4096
        loss_cfg:
          target: torch.nn.Identity

    cond_stage_config:
      target: michelangelo.models.conditional_encoders.encoder_factory.DinoImageEmbedder
      params:
        version: "facebook/dinov2-large"
        zero_embedding_radio: 0.1
        image_size: 224

    # clip_cond_stage_config:
    #   target: michelangelo.models.conditional_encoders.encoder_factory.FrozenCLIPImageGridEmbedder
    #   params:
    #     version: "openai/clip-vit-large-patch14"
    #     zero_embedding_radio: 0.1
    #     image_size: 224

    first_stage_key: "surface"
    cond_stage_key: "image"
    incomplete_points_key: "incomplete_points"
    scale_by_std: false

    denoiser_cfg:
      target: michelangelo.models.asl_diffusion.asl_udt.ConditionalASLUDTDenoiser
      params:
        input_channels: 137 # 128 + 3 + 3 +  (outdim, uvw, incomplete points)
        output_channels: 3
        n_ctx: 4096
        width: 768
        layers: 6   # 2 * 6 + 1 = 13
        heads: 12
        context_dim: 1024 #832 # 768 + 64
        init_scale: 1.0
        skip_ln: true
        use_checkpoint: true

    scheduler_cfg:
      guidance_scale: 7.5
      num_inference_steps: 100
      eta: 0.0

      noise:
        target: diffusers.schedulers.DDPMScheduler
        params:
          num_train_timesteps: 1000
          beta_start: 0.00085
          beta_end: 0.012
          beta_schedule: "scaled_linear"
          variance_type: "fixed_small"
          clip_sample: false
      denoise:
        target: diffusers.schedulers.DDIMScheduler
        params:
          num_train_timesteps: 1000
          beta_start: 0.00085
          beta_end: 0.012
          beta_schedule: "scaled_linear"
          clip_sample: false   # clip sample to -1~1
          set_alpha_to_one: false
          steps_offset: 1

    optimizer_cfg:
      optimizer:
        target: torch.optim.AdamW
        params:
          betas: [0.9, 0.99]
          lr: 1.e-4
          eps: 1.e-4
          weight_decay: 1.e-2

      # scheduler:
      #   target: torch.optim.lr_scheduler.CosineAnnealingLR
      #   params:
      #     T_max: 500
      #     eta_min: 0.0
      #     last_epoch: -1

    loss_cfg:
      loss_type: "mse"

data:
  data_dir: "~/Documents/datasets/ShapeNetViPC-Dataset"
  num_workers: 4
  view_align: true
  category: "plane"
  mini: true
  image_size: 224


dino_cond_stage_config:
  target: michelangelo.models.conditional_encoders.encoder_factory.DinoImageEmbedder
  params:
    version: "facebook/dinov2-large"
    zero_embedding_radio: 0.1
    image_size: 224

clip_cond_stage_config:
  target: michelangelo.models.conditional_encoders.encoder_factory.FrozenCLIPImageGridEmbedder
  params:
    version: "openai/clip-vit-large-patch14"
    zero_embedding_radio: 0.1
    image_size: 224

# training params
batch_size: 8
val_batch_size: 8
max_epochs: 2000
strategy: ddp_find_unused_parameters_false
devices: -1
use_ckpt: false
use_swa: false
use_lr_finder: false
max_lr: 1.e-3 # if lr_finder is true, this will be used as max_lr
log_every_n_steps: 10
accumulate_grad_batches: 4
gradient_clip_val: 0.5
check_val_every_n_epoch: 30  # Run validation once per 5 epochs
fast_dev_run: false
limit_train_batches: null
limit_val_batches: null
overfit_batches: 0.0
resume_from_ckpt: null
precision: "16"
model:
  target: michelangelo.models.tsal.asl_pl_module.AlignedShapeAsLatentPLModule
  params:
    # ckpt_path: #"checkpoints/aligned_shape_latents/shapevae-256.ckpt"
    shape_module_cfg:
      target: michelangelo.models.tsal.sal_perceiver.AlignedShapeLatentPerceiver
      params:
        num_latents: 256
        embed_dim: 64
        point_feats: 3   # normal
        num_freqs: 8
        include_pi: false
        heads: 12
        width: 768
        num_encoder_layers: 8
        num_decoder_layers: 16
        use_ln_post: true
        init_scale: 0.25
        qkv_bias: false
        use_checkpoint: true
    aligned_module_cfg:
      target: michelangelo.models.tsal.clip_asl_module.CLIPAlignedShapeAsLatentModule
      params:
        clip_model_version: "openai/clip-vit-large-patch14"
    
    dropout: 0.1
    num_decoder_layers_cross_attn: 2

    loss_cfg:
      target: michelangelo.models.tsal.loss.ContrastKLNearFar
      params:
        contrast_weight: 0.1
        chamfer_weight: 1.0 #0.1
        mse_weight: 0.0 #0.2
        # near_weight: 0.1
        kl_weight: 0.001

    optimizer_cfg:
      optimizer:
        target: torch.optim.AdamW
        params:
          betas: [0.9, 0.99]
          # eps: 1.e-6
          lr: 1.e-4 #1.e-3
          weight_decay: 1.e-2

      # scheduler:
      #   target: michelangelo.utils.lr_scheduler.LambdaWarmUpCosineFactorScheduler
      #   params:
      #     warm_up_steps: 150
      #     f_start: 1.e-8
      #     f_min: 1.e-6
      #     f_max: 1.e-6
      scheduler:
        target: torch.optim.lr_scheduler.ReduceLROnPlateau
        params:
          mode: "min"
          factor: 0.1
          patience: 10
          # verbose: true
data:
  data_dir: "~/Documents/datasets/ShapeNetViPC-Dataset"
  num_workers: 0
  view_align: true
  category: "plane"
  mini: true
  image_size: 224


# training params
batch_size: 4
max_epochs: -1
use_ckpt: false
use_swa: true
use_lr_finder: false
max_lr: 1.e-3 # if lr_finder is true, this will be used as max_lr
log_every_n_steps: 50
accumulate_grad_batches: 4
gradient_clip_val: 0.5
check_val_every_n_epoch: 100  # Run validation once per 5 epochs
fast_dev_run: false
limit_train_batches: null
limit_val_batches: null
overfit_batches: 1 #0.0
resume_from_ckpt: null
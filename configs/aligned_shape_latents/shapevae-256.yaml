model:
  target: michelangelo.models.tsal.asl_pl_module.AlignedShapeAsLatentPLModule
  params:
    ckpt_path: "checkpoints/aligned_shape_latents/shapevae-256.ckpt"
    shape_module_cfg:
      target: michelangelo.models.tsal.sal_perceiver.AlignedShapeLatentPerceiver
      params:
        num_latents: 256
        embed_dim: 64
        point_feats: 3   # normal
        num_freqs: 8
        include_pi: false
        heads: 12
        width: 768
        num_encoder_layers: 8
        num_decoder_layers: 16
        use_ln_post: true
        init_scale: 0.25
        qkv_bias: false
        use_checkpoint: true
    aligned_module_cfg:
      target: michelangelo.models.tsal.dino_asl_module.DinoAlignedShapeAsLatentModule
      params:
        clip_model_version: "facebook/dinov2-large"
    
    dropout: 0.1
    num_decoder_layers_cross_attn: 2
    numpoints: 4096

    loss_cfg:
      target: michelangelo.models.tsal.loss.ContrastKLNearFar
      params:
        contrast_weight: 0.1
        chamfer_weight: 0.2
        mse_weight: 0.0 #0.2
        # near_weight: 0.1
        kl_weight: 0.001

    optimizer_cfg:
      optimizer:
        target: torch.optim.AdamW
        params:
          betas: [0.9, 0.99]
          # eps: 1.e-6
          lr: 1.e-3
          weight_decay: 1.e-2
      
      scheduler:
        target: torch.optim.lr_scheduler.ReduceLROnPlateau
        params:
          mode: 'min'
          factor: 0.1
          patience: 10
          min_lr: 1e-6
          eps: 1e-08

      # scheduler:
      #   target: torch.optim.lr_scheduler.CosineAnnealingLR
      #   params:
      #     T_max: 500
      #     eta_min: 0.0
      #     last_epoch: -1
data:
  data_dir: "~/Documents/datasets/ShapeNetViPC-Dataset"
  num_workers: 2
  view_align: true
  category: "plane"
  mini: true
  image_size: 224


# training params
batch_size: 32
strategy: ddp_find_unused_parameters_false
devices: -1
max_epochs: 500
use_ckpt: false
use_swa: true
use_lr_finder: false
max_lr: 1.e-3 # if lr_finder is true, this will be used as max_lr
log_every_n_steps: 100
accumulate_grad_batches: 4
gradient_clip_val: 0.5
check_val_every_n_epoch: 30  # Run validation once per 5 epochs
fast_dev_run: false
limit_train_batches: null
limit_val_batches: null
overfit_batches: 0.0
resume_from_ckpt: null